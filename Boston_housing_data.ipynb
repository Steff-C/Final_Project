{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Keras specific\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv using pandas. We should specify the column index to avoid mislabelling problems when data is uploaded\n",
    "data=pd.read_csv('data/housingdata.csv',index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head to check the first few columns of the data set\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We rename the columns now using data.columns function\n",
    "data.columns=['Crime Rate','Residential Land Zone','Non-retail business acres','Charles River Variable','Nitric Oxide Concentration',\n",
    "             'Rooms','Age','Distance','Accessiblity to Highway','Tax Rate','Pupil-Teacher ratio','Proportion of Blacks','% lower status',\n",
    "             'Median Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head after renaming the columns\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop proportion of blacks\n",
    "# We use data.drop to drop the proportion of blacks\n",
    "data.drop(\"Proportion of Blacks\",axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head to verify\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use describe for sample stats and central tendency stats\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use data.info to get the data types and count of non-nulls in the dataset\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the result above, we found out that there is a possibility of nulls or missing data which we can count by summing all nulls\n",
    "# checking for nulls\n",
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's check for a correlation between the target value, which is our Median House price values, and all the other columns.\n",
    "# Let's check then for a correlation to our median value (which of the variables are highly correlated to it?)\n",
    "# We use seaborn heatmap. We should also consider the effects of outliers as well. But, let's check for correlation first\n",
    "# Using seaborn\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(data.corr(),annot = True,cmap= 'coolwarm', linewidths=1, linecolor='white',fmt='.2g')\n",
    "plt.savefig(\"output/Seaborn.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the last column, we found out that the # of rooms (0.7) and % lower status (-0.74) have a strong correlation to the median house value. This means they directly influence the price. The correlation analysis shows that median house value is highly correlated to % lower status and the number of rooms per dwelling.\n",
    "The total number of Rooms is positively correlated to Median Value. So as number of Rooms increases, the Median value increases. The opposite is true for % lower status: when % lower status goes up, the price goes down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [

    "data_dropna = data.copy()\n",
    "data_dropna = data_dropna.dropna()\n",

    "#data.info()\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "data.iloc[:,:]=imputer.fit_transform(data)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.dropna(subset=['Nitric Oxide Concentration', 'Rooms'])\n",
    "#data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.fillna(0)\n",
    "#data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the data to X and y\n",
    "X = data[['Crime Rate','Residential Land Zone','Non-retail business acres','Charles River Variable','Nitric Oxide Concentration',\n",
    "             'Rooms','Age','Distance','Accessiblity to Highway','Tax Rate','Pupil-Teacher ratio','% lower status']]\n",
    "y = data['Median Value'].values.reshape(-1,1)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to create training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model using LinearRegression\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data and calculate the scores for the training and testing data\n",
    "model.fit(X_train, y_train)\n",
    "training_score = model.score(X_train, y_train)\n",
    "testing_score = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training Score: {training_score}\")\n",
    "print(f\"Testing Score: {testing_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Residuals for the Training and Testing data\n",
    "plt.scatter(model.predict(X_train), model.predict(X_train) - y_train, c=\"blue\", label=\"Training Data\")\n",
    "plt.scatter(model.predict(X_test), model.predict(X_test) - y_test, c=\"orange\", label=\"Testing Data\")\n",
    "plt.legend()\n",
    "plt.hlines(y=0, xmin=y.min(), xmax=y.max())\n",
    "plt.title(\"Residual Plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seloms Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputate nulls or missing values using the simpleimputer from sklearn and transform them to a median by column \n",
    "from sklearn.impute import SimpleImputer\n",
    "data_copy = data.copy()\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "data_copy.iloc[:,:]=imputer.fit_transform(data_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for nulls or missing values\n",
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting nulls or missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stephs Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Dataframe to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'data\\revisedhousingdata.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rachels Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "data['Home Value'] = (data['Median Value'] * 1000)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a scatter plot of Charles River Variable(distance from the river) vs. value\n",
    "crimeRate = data.iloc[:,0]\n",
    "value = data.iloc[:,13]\n",
    "\n",
    "plt.scatter(crimeRate, value, color = '#545E45')\n",
    "\n",
    "#label\n",
    "plt.title('Crime Rate vs Value')\n",
    "plt.xlabel('Crime Rate')\n",
    "plt.ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a scatter plot of Charles River Variable(distance from the river) vs. value\n",
    "charlesRiver = data.iloc[:,3]\n",
    "value = data.iloc[:,13]\n",
    "\n",
    "plt.scatter(charlesRiver, value, color = '#8D2B00')\n",
    "\n",
    "#label\n",
    "plt.title('Distance from Charles River vs Value')\n",
    "plt.xlabel('Distance')\n",
    "plt.ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a scatter plot of Nitric Oxide Concentration vs. value\n",
    "nitricOxide = data.iloc[:,4]\n",
    "value = data.iloc[:,13]\n",
    "\n",
    "plt.scatter(nitricOxide, value, color = '#BE6731')\n",
    "\n",
    "#label\n",
    "plt.title('Nitric Oxide vs Value')\n",
    "plt.xlabel('Nitric')\n",
    "plt.ylabel('Value')\n",
    "plt.savefig(\"output/NitricOxideVsValue.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a scatter plot of rooms vs. value\n",
    "rooms = data.iloc[:,5]\n",
    "value = data.iloc[:,13]\n",
    "\n",
    "plt.scatter(rooms, value, color = '#76704C')\n",
    "\n",
    "\n",
    "#label\n",
    "plt.title('Number of Rooms vs Value')\n",
    "plt.xlabel('Rooms')\n",
    "plt.ylabel('Value')\n",
    "plt.savefig(\"output/NumberOfRoomsVsValue.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a scatter plot of age vs. value\n",
    "age = data.iloc[:,6]\n",
    "value = data.iloc[:,13]\n",
    "\n",
    "plt.scatter(age, value, color = '#B55119')\n",
    "\n",
    "#label\n",
    "plt.title('Age of Home vs Value')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Value')\n",
    "plt.savefig(\"output/AgeVsValue.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a scatter plot of % lower status vs. value\n",
    "lowerStatus = data.iloc[:,11]\n",
    "value = data.iloc[:,13]\n",
    "\n",
    "plt.scatter(lowerStatus, value, color = '#1F2D16')\n",
    "\n",
    "#label\n",
    "plt.title('% Lower Status vs Value')\n",
    "plt.xlabel('Status')\n",
    "plt.ylabel('Value')\n",
    "plt.savefig(\"output/StatusVsValue.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matts Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Model Before Standardizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from math import sqrt\n",
    "\n",
    "# Keras specific\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the data to X and y\n",
    "X = data[['Crime Rate','Residential Land Zone','Non-retail business acres','Charles River Variable','Nitric Oxide Concentration',\n",
    "             'Rooms','Age','Distance','Accessiblity to Highway','Tax Rate','Pupil-Teacher ratio','% lower status']]\n",
    "y = data['Median Value'].values.reshape(-1,1)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to create training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=150)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the model\n",
    "model = Sequential()\n",
    "model.add(Dense(36, input_dim=12, activation= \"relu\"))\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
    "model.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a test preciction to see if we're on the right track\n",
    "row = 3\n",
    "test_row = X_test.iloc[row, :]\n",
    "test_row_array = np.array(test_row).reshape(1, 12)\n",
    "test_row_array.shape\n",
    "\n",
    "print(f'Prediction = {model.predict(test_row_array)}')\n",
    "print(f'Actual = {y_test[row]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],

   "source": [
    "#Use X_test to predict y, and use that to calculate our R score\n",
    "y_test_predict = model.predict(X_test)\n",
    "r2_score(y_test, y_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],

   "source": [
    "model.evaluate(X_test,y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R2 score for our model is not very reliable at a .37 which would put it in the higher of the of low reliability range for a model. Next we will standarize our data and then see if that improves our R score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model With Standardized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = StandardScaler().fit(X_train)\n",
    "y_scaler = StandardScaler().fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "y_train_scaled = y_scaler.transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {
    "scrolled": true
   },

   "outputs": [],
   "source": [
    "model_scaled = Sequential()\n",
    "model_scaled.add(Dense(36, input_dim=12, activation= \"relu\"))\n",
    "model_scaled.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_scaled.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
    "model_scaled.fit(X_train_scaled, y_train_scaled, epochs=100)\n",
    "\n",
    "#r2_score(y_train_scaled, y_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use X_test to predict y, and use that to calculate our R score\n",
    "y_test_scaled_predict = model_scaled.predict(X_test_scaled)\n",
    "r2_score(y_test_scaled, y_test_scaled_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model\n",
    "model_scaled.evaluate(X_test_scaled,y_test_scaled, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the standardized data made a huge impact on improving the performance of our model, we still thought it could be further improved. In order to try making it more accurate we decided to drop some columns that had low correlation to the median household income to see if that would make a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manipulated_data = data.copy()\n",
    "\n",
    "manipulated_data = manipulated_data.drop([\"Charles River Variable\", 'Distance'],axis=1)\n",
    "manipulated_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_manipulated = manipulated_data[['Crime Rate','Residential Land Zone','Non-retail business acres','Nitric Oxide Concentration',\n",
    "             'Rooms','Age','Accessiblity to Highway','Tax Rate','Pupil-Teacher ratio','% lower status']]\n",
    "y_manipulated = manipulated_data['Median Value'].values.reshape(-1,1)\n",
    "#print(X.shape, y.shape)\n",
    "\n",
    "X_train_manipulated, X_test_manipulated, y_train_manipulated, y_test_manipulated = train_test_split(X_manipulated, y_manipulated, random_state=150)\n",
    "#print(X_train.shape)\n",
    "#print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler_manipulated = StandardScaler().fit(X_train_manipulated)\n",
    "y_scaler_manipulated = StandardScaler().fit(y_train_manipulated)\n",
    "\n",
    "X_train_scaled_manipulated = X_scaler_manipulated.transform(X_train_manipulated)\n",
    "X_test_scaled_manipulated = X_scaler_manipulated.transform(X_test_manipulated)\n",
    "y_train_scaled_manipulated = y_scaler_manipulated.transform(y_train_manipulated)\n",
    "y_test_scaled_manipulated = y_scaler_manipulated.transform(y_test_manipulated)\n",
    "\n",
    "model_manipulated = Sequential()\n",
    "model_manipulated.add(Dense(30, input_dim=10, activation= \"relu\"))\n",
    "\n",
    "model_manipulated.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_manipulated.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
    "model_manipulated.fit(X_train_scaled_manipulated, y_train_scaled_manipulated, epochs=100)\n",
    "\n",
    "model_manipulated.evaluate(X_test_scaled_manipulated,y_test_scaled_manipulated, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use X_test to predict y, and use that to calculate our R score\n",
    "y_test_scaled_manip_predict = model_manipulated.predict(X_test_scaled_manipulated)\n",
    "r2_score(y_test_scaled_manipulated, y_test_scaled_manip_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the two rows did not make any significnat impact on the r score of the model. Next we will try cherry picking the rows with the highest correlation to median value to see if that changes at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cherrypicked = data[['Non-retail business acres','Nitric Oxide Concentration',\n",
    "             'Rooms','Tax Rate','Pupil-Teacher ratio','% lower status']]\n",
    "y_cherrypicked = data['Median Value'].values.reshape(-1,1)\n",
    "#print(X.shape, y.shape)\n",
    "\n",
    "X_train_cherrypicked, X_test_cherrypicked, y_train_cherrypicked, y_test_cherrypicked = train_test_split(X_cherrypicked, y_cherrypicked, random_state=150)\n",
    "#print(X_train.shape)\n",
    "#print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler_cherrypicked = StandardScaler().fit(X_train_cherrypicked)\n",
    "y_scaler_cherrypicked = StandardScaler().fit(y_train_cherrypicked)\n",
    "\n",
    "X_train_scaled_cherrypicked = X_scaler_cherrypicked.transform(X_train_cherrypicked)\n",
    "X_test_scaled_cherrypicked = X_scaler_cherrypicked.transform(X_test_cherrypicked)\n",
    "y_train_scaled_cherrypicked = y_scaler_cherrypicked.transform(y_train_cherrypicked)\n",
    "y_test_scaled_cherrypicked = y_scaler_cherrypicked.transform(y_test_cherrypicked)\n",
    "\n",
    "model_cherrypicked = Sequential()\n",
    "model_cherrypicked.add(Dense(18, input_dim=6, activation= \"relu\"))\n",
    "model_cherrypicked.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_cherrypicked.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
    "model_cherrypicked.fit(X_train_scaled_cherrypicked, y_train_scaled_cherrypicked, epochs=100)\n",
    "\n",
    "model_cherrypicked.evaluate(X_test_scaled_cherrypicked,y_test_scaled_cherrypicked, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use X_test to predict y, and use that to calculate our R score\n",
    "y_test_scaled_cherrypicked_predict = model_cherrypicked.predict(X_test_scaled_cherrypicked)\n",
    "r2_score(y_test_scaled_cherrypicked, y_test_scaled_cherrypicked_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cherry picking the rows we wanted to use had a adverse impact on our models r score. Overall manipulating the data further does not seem to be helping to improve the r score of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have shown manipulating which rows we used in our data did not have a significnat impact, we wanted to compare two methods of cleaning our data to see if one is more reliable than another. The models above used imputation where we replaced the null values in our data with the mean of the column that null value is in. \n",
    "\n",
    "Below is a model using a data set where we instead drop the rows contianing nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_drop = data_dropna[['Crime Rate','Residential Land Zone','Non-retail business acres','Charles River Variable','Nitric Oxide Concentration',\n",
    "             'Rooms','Age','Distance','Accessiblity to Highway','Tax Rate','Pupil-Teacher ratio','% lower status']]\n",
    "y_drop = data_dropna['Median Value'].values.reshape(-1,1)\n",
    "\n",
    "X_train_drop, X_test_drop, y_train_drop, y_test_drop = train_test_split(X_drop, y_drop, random_state=150)\n",
    "\n",
    "X_scaler_drop = StandardScaler().fit(X_train_drop)\n",
    "y_scaler_drop = StandardScaler().fit(y_train_drop)\n",
    "\n",
    "X_train_scaled_drop = X_scaler_drop.transform(X_train_drop)\n",
    "X_test_scaled_drop = X_scaler_drop.transform(X_test_drop)\n",
    "y_train_scaled_drop = y_scaler_drop.transform(y_train_drop)\n",
    "y_test_scaled_drop = y_scaler_drop.transform(y_test_drop)\n",
    "\n",
    "model_drop = Sequential()\n",
    "model_drop.add(Dense(36, input_dim=12, activation= \"relu\"))\n",
    "model_drop.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_drop.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
    "model_drop.fit(X_train_scaled_drop, y_train_scaled_drop, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_scaled_predict_drop = model_drop.predict(X_test_scaled_drop)\n",
    "r2_score(y_test_scaled_drop, y_test_scaled_predict_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Drop NA we actually see that the model becomes far more accurate, up to .9 from .75. This further supports avoiding unessecary manpulation of the data. Next we will try removing outliers from our data set to see if that improves our models performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "data_remove_outliers = data_dropna.copy()\n",
    "data_remove_outliers = data_remove_outliers[(np.abs(stats.zscore(data_remove_outliers)) < 3).all(axis=1)]\n",
    "data_remove_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_outlier = data_remove_outliers[['Crime Rate','Residential Land Zone','Non-retail business acres','Charles River Variable','Nitric Oxide Concentration',\n",
    "             'Rooms','Age','Distance','Accessiblity to Highway','Tax Rate','Pupil-Teacher ratio','% lower status']]\n",
    "y_outlier = data_remove_outliers['Median Value'].values.reshape(-1,1)\n",
    "\n",
    "X_train_outlier, X_test_outlier, y_train_outlier, y_test_outlier = train_test_split(X_outlier, y_outlier, random_state=150)\n",
    "\n",
    "X_scaler_outlier = StandardScaler().fit(X_train_outlier)\n",
    "y_scaler_outlier = StandardScaler().fit(y_train_outlier)\n",
    "\n",
    "X_train_scaled_outlier = X_scaler_outlier.transform(X_train_outlier)\n",
    "X_test_scaled_outlier = X_scaler_outlier.transform(X_test_outlier)\n",
    "y_train_scaled_outlier = y_scaler_outlier.transform(y_train_outlier)\n",
    "y_test_scaled_outlier = y_scaler_outlier.transform(y_test_outlier)\n",
    "\n",
    "model_outlier = Sequential()\n",
    "model_outlier.add(Dense(36, input_dim=12, activation= \"relu\"))\n",
    "model_outlier.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_outlier.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
    "model_outlier.fit(X_train_scaled_outlier, y_train_scaled_outlier, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_scaled_predict_outlier = model_outlier.predict(X_test_scaled_outlier)\n",
    "r2_score(y_test_scaled_outlier, y_test_scaled_predict_outlier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing outliers from our data has improved its overall R2 score slightly, which would indicate it is helpful in improving the reliability of the model overall."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
