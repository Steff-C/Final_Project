{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Keras specific\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv using pandas. We should specify the column index to avoid mislabelling problems when data is uploaded\n",
    "data=pd.read_csv('data/housingdata.csv',index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head to check the first few columns of the data set\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We rename the columns now using data.columns function\n",
    "data.columns=['Crime Rate','Residential Land Zone','Non-retail business acres','Charles River Variable','Nitric Oxide Concentration',\n",
    "             'Rooms','Age','Distance','Accessiblity to Highway','Tax Rate','Pupil-Teacher ratio','Proportion of Blacks','% lower status',\n",
    "             'Median Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head after renaming the columns\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop proportion of blacks\n",
    "# We use data.drop to drop the proportion of blacks\n",
    "data.drop(\"Proportion of Blacks\",axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head to verify\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use describe for sample stats and central tendency stats\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use data.info to get the data types and count of non-nulls in the dataset\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the result above, we found out that there is a possibility of nulls or missing data which we can count by summing all nulls\n",
    "# checking for nulls\n",
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's check for a correlation between the target value, which is our Median House price values, and all the other columns.\n",
    "# Let's check then for a correlation to our median value (which of the variables are highly correlated to it?)\n",
    "# We use seaborn heatmap. We should also consider the effects of outliers as well. But, let's check for correlation first\n",
    "# Using seaborn\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(data.corr(),annot = True,cmap= 'coolwarm', linewidths=1, linecolor='white',fmt='.2g')\n",
    "plt.savefig(\"output/Seaborn.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the last column, we found out that the # of rooms (0.7) and % lower status (-0.74) have a strong correlation to the median house value. This means they directly influence the price. The correlation analysis shows that median house value is highly correlated to % lower status and the number of rooms per dwelling.\n",
    "The total number of Rooms is positively correlated to Median Value. So as number of Rooms increases, the Median value increases. The opposite is true for % lower status: when % lower status goes up, the price goes down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.dropna()\n",
    "#data.info()\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "data.iloc[:,:]=imputer.fit_transform(data)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.dropna(subset=['Nitric Oxide Concentration', 'Rooms'])\n",
    "#data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.fillna(0)\n",
    "#data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the data to X and y\n",
    "X = data[['Crime Rate','Residential Land Zone','Non-retail business acres','Charles River Variable','Nitric Oxide Concentration',\n",
    "             'Rooms','Age','Distance','Accessiblity to Highway','Tax Rate','Pupil-Teacher ratio','% lower status']]\n",
    "y = data['Median Value'].values.reshape(-1,1)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to create training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model using LinearRegression\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data and calculate the scores for the training and testing data\n",
    "model.fit(X_train, y_train)\n",
    "training_score = model.score(X_train, y_train)\n",
    "testing_score = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training Score: {training_score}\")\n",
    "print(f\"Testing Score: {testing_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Residuals for the Training and Testing data\n",
    "plt.scatter(model.predict(X_train), model.predict(X_train) - y_train, c=\"blue\", label=\"Training Data\")\n",
    "plt.scatter(model.predict(X_test), model.predict(X_test) - y_test, c=\"orange\", label=\"Testing Data\")\n",
    "plt.legend()\n",
    "plt.hlines(y=0, xmin=y.min(), xmax=y.max())\n",
    "plt.title(\"Residual Plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seloms Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputate nulls or missing values using the simpleimputer from sklearn and transform them to a median by column \n",
    "from sklearn.impute import SimpleImputer\n",
    "data_copy = data.copy()\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "data_copy.iloc[:,:]=imputer.fit_transform(data_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for nulls or missing values\n",
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting nulls or missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stephs Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Dataframe to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'data\\revisedhousingdata.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rachels Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "data['Home Value'] = (data['Median Value'] * 1000)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a scatter plot of Charles River Variable(distance from the river) vs. value\n",
    "crimeRate = data.iloc[:,0]\n",
    "value = data.iloc[:,13]\n",
    "\n",
    "plt.scatter(crimeRate, value, color = '#545E45')\n",
    "\n",
    "#label\n",
    "plt.title('Crime Rate vs Value')\n",
    "plt.xlabel('Crime Rate')\n",
    "plt.ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a scatter plot of Charles River Variable(distance from the river) vs. value\n",
    "charlesRiver = data.iloc[:,3]\n",
    "value = data.iloc[:,13]\n",
    "\n",
    "plt.scatter(charlesRiver, value, color = '#8D2B00')\n",
    "\n",
    "#label\n",
    "plt.title('Distance from Charles River vs Value')\n",
    "plt.xlabel('Distance')\n",
    "plt.ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a scatter plot of Nitric Oxide Concentration vs. value\n",
    "nitricOxide = data.iloc[:,4]\n",
    "value = data.iloc[:,13]\n",
    "\n",
    "plt.scatter(nitricOxide, value, color = '#BE6731')\n",
    "\n",
    "#label\n",
    "plt.title('Nitric Oxide vs Value')\n",
    "plt.xlabel('Nitric')\n",
    "plt.ylabel('Value')\n",
    "plt.savefig(\"output/NitricOxideVsValue.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a scatter plot of rooms vs. value\n",
    "rooms = data.iloc[:,5]\n",
    "value = data.iloc[:,13]\n",
    "\n",
    "plt.scatter(rooms, value, color = '#76704C')\n",
    "\n",
    "\n",
    "#label\n",
    "plt.title('Number of Rooms vs Value')\n",
    "plt.xlabel('Rooms')\n",
    "plt.ylabel('Value')\n",
    "plt.savefig(\"output/NumberOfRoomsVsValue.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a scatter plot of age vs. value\n",
    "age = data.iloc[:,6]\n",
    "value = data.iloc[:,13]\n",
    "\n",
    "plt.scatter(age, value, color = '#B55119')\n",
    "\n",
    "#label\n",
    "plt.title('Age of Home vs Value')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Value')\n",
    "plt.savefig(\"output/AgeVsValue.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a scatter plot of % lower status vs. value\n",
    "lowerStatus = data.iloc[:,11]\n",
    "value = data.iloc[:,13]\n",
    "\n",
    "plt.scatter(lowerStatus, value, color = '#1F2D16')\n",
    "\n",
    "#label\n",
    "plt.title('% Lower Status vs Value')\n",
    "plt.xlabel('Status')\n",
    "plt.ylabel('Value')\n",
    "plt.savefig(\"output/StatusVsValue.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matts Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Keras specific\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the data to X and y\n",
    "X = data[['Crime Rate','Residential Land Zone','Non-retail business acres','Charles River Variable','Nitric Oxide Concentration',\n",
    "             'Rooms','Age','Distance','Accessiblity to Highway','Tax Rate','Pupil-Teacher ratio','% lower status']]\n",
    "y = data['Median Value'].values.reshape(-1,1)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to create training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=150)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the model\n",
    "model = Sequential()\n",
    "model.add(Dense(36, input_dim=12, activation= \"relu\"))\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
    "model.fit(X_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_train= model.predict(X_train)\n",
    "#print(np.sqrt(mean_squared_error(y_train,pred_train)))\n",
    "\n",
    "#pred= model.predict(X_test)\n",
    "#print(np.sqrt(mean_squared_error(y_test,pred))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a MSE of 78 our model is clearly not currently very reliable or accurate. One of the first things that could be causing this is the fluctuations in data that we are currently using. As such we need to standardize our data so that everything is being compared equally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = StandardScaler().fit(X_train)\n",
    "y_scaler = StandardScaler().fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "y_train_scaled = y_scaler.transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scaled = Sequential()\n",
    "model_scaled.add(Dense(36, input_dim=12, activation= \"relu\"))\n",
    "model_scaled.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_scaled.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
    "model_scaled.fit(X_train, y_train, epochs=20)\n",
    "\n",
    "model_scaled.evaluate(X_test_scaled,y_test_scaled, verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the standardized data made a huge impact on improving the performance of our model, we still thought it could be further improved. In order to try making it more accurate we decided to drop some columns that had low correlation to the median household income to see if that would make a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manipulated_data = data.copy()\n",
    "\n",
    "manipulated_data = manipulated_data.drop([\"Charles River Variable\", 'Distance'],axis=1)\n",
    "manipulated_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_manipulated = data[['Crime Rate','Residential Land Zone','Non-retail business acres','Nitric Oxide Concentration',\n",
    "             'Rooms','Age','Accessiblity to Highway','Tax Rate','Pupil-Teacher ratio','% lower status']]\n",
    "y_manipulated = data['Median Value'].values.reshape(-1,1)\n",
    "#print(X.shape, y.shape)\n",
    "\n",
    "X_train_manipulated, X_test_manipulated, y_train_manipulated, y_test_manipulated = train_test_split(X_manipulated, y_manipulated, random_state=150)\n",
    "#print(X_train.shape)\n",
    "#print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler_manipulated = StandardScaler().fit(X_train_manipulated)\n",
    "y_scaler_manipulated = StandardScaler().fit(y_train_manipulated)\n",
    "\n",
    "X_train_scaled_manipulated = X_scaler_manipulated.transform(X_train_manipulated)\n",
    "X_test_scaled_manipulated = X_scaler_manipulated.transform(X_test_manipulated)\n",
    "y_train_scaled_manipulated = y_scaler_manipulated.transform(y_train_manipulated)\n",
    "y_test_scaled_manipulated = y_scaler_manipulated.transform(y_test_manipulated)\n",
    "\n",
    "model_scaled = Sequential()\n",
    "model_scaled.add(Dense(30, input_dim=10, activation= \"relu\"))\n",
    "model_scaled.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_scaled.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
    "model_scaled.fit(X_train_manipulated, y_train_manipulated, epochs=20)\n",
    "\n",
    "model_scaled.evaluate(X_test_scaled_manipulated,y_test_scaled_manipulated, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the variables we removed had low correlation to the median value, removing them significantly worsened our data set. In order to see if this held true we tried selecting the 6 criteria with the highest corelation to see if it would improve, or worsen the performance of the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_manipulated = data[['Non-retail business acres','Nitric Oxide Concentration',\n",
    "             'Rooms','Tax Rate','Pupil-Teacher ratio','% lower status']]\n",
    "y_manipulated = data['Median Value'].values.reshape(-1,1)\n",
    "#print(X.shape, y.shape)\n",
    "\n",
    "X_train_manipulated, X_test_manipulated, y_train_manipulated, y_test_manipulated = train_test_split(X_manipulated, y_manipulated, random_state=150)\n",
    "#print(X_train.shape)\n",
    "#print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler_manipulated = StandardScaler().fit(X_train_manipulated)\n",
    "y_scaler_manipulated = StandardScaler().fit(y_train_manipulated)\n",
    "\n",
    "X_train_scaled_manipulated = X_scaler_manipulated.transform(X_train_manipulated)\n",
    "X_test_scaled_manipulated = X_scaler_manipulated.transform(X_test_manipulated)\n",
    "y_train_scaled_manipulated = y_scaler_manipulated.transform(y_train_manipulated)\n",
    "y_test_scaled_manipulated = y_scaler_manipulated.transform(y_test_manipulated)\n",
    "\n",
    "model_scaled = Sequential()\n",
    "model_scaled.add(Dense(30, input_dim=6, activation= \"relu\"))\n",
    "model_scaled.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_scaled.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
    "model_scaled.fit(X_train_manipulated, y_train_manipulated, epochs=20)\n",
    "\n",
    "model_scaled.evaluate(X_test_scaled_manipulated,y_test_scaled_manipulated, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the models performance did drop, it did not drop as much as anticipated. Overall removing the data from the dataset has had a consistently negative impact on the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
