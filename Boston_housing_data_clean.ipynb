{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of our project was to see the Boston housing data could reliably predict the median house price utilizing machine learning and Python. We also wanted to see if filtering out different values and inputs could help to improve the modelâ€™s efficiency and performance, and if we could manipulate the data to provide more accurate estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Keras specific\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and Cleaning Data\n",
    "\n",
    "First we had to import the data in our Jupyter Notebook, then we went about cleaning it. First we renamed the columns in our data set to be more intuitive, making sure we could read what we were looking at without confusion. Then we checked to see if any rows had null/blank values, which we did find several of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv using pandas. We should specify the column index to avoid mislabelling problems when data is uploaded\n",
    "data=pd.read_csv('data/housingdata.csv',index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head to check the first few columns of the data set\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We rename the columns now using data.columns function\n",
    "data.columns=['Crime Rate','Residential Land Zone','Non-retail business acres','Charles River Variable','Nitric Oxide Concentration',\n",
    "             'Rooms','Age','Distance','Accessiblity to Highway','Tax Rate','Pupil-Teacher ratio','Proportion of Blacks','% lower status',\n",
    "             'Median Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head after renaming the columns\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop proportion of blacks\n",
    "# We use data.drop to drop the proportion of blacks\n",
    "data.drop(\"Proportion of Blacks\",axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head to verify\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use describe for sample stats and central tendency stats\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use data.info to get the data types and count of non-nulls in the dataset\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the result above, we found out that there is a possibility of nulls or missing data which we can count by summing all nulls\n",
    "# checking for nulls\n",
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we had two thoughts on how to handle the null values. One was to simply drop the rows with null values and continue, and the other was imputation. Meaning adding in the median value of a given column in place of each null value. After discussing we decided to run the model we were building on both methods and see which performed better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dropna = data.copy()\n",
    "data_dropna = data_dropna.dropna()\n",
    "#data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "data.iloc[:,:]=imputer.fit_transform(data)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for nulls or missing values\n",
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting nulls or missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis\n",
    "\n",
    "Once we had cleaned up the data using both methodologies, we moved on to some early analysis. One of the first things we did was use a Seaborn Heatmap to see which metrics had high correlation with media house value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's check for a correlation between the target value, which is our Median House price values, and all the other columns.\n",
    "# Let's check then for a correlation to our median value (which of the variables are highly correlated to it?)\n",
    "# We use seaborn heatmap. We should also consider the effects of outliers as well. But, let's check for correlation first\n",
    "# Using seaborn\n",
    "plt.figure(figsize=(18, 18))\n",
    "sns.heatmap(data.corr(),annot = True,cmap= 'coolwarm', linewidths=1, linecolor='white',fmt='.2g')\n",
    "plt.savefig(\"output/Seaborn.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the last column, we found out that the # of rooms (0.7) and % lower status (-0.74) have a strong correlation to the median house value. This means they directly influence the price. The correlation analysis shows that median house value is highly correlated to % lower status and the number of rooms per dwelling.\n",
    "The total number of Rooms is positively correlated to Median Value. So as number of Rooms increases, the Median value increases. The opposite is true for % lower status: when % lower status goes up, the price goes down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stephs Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Dataframe to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'data\\revisedhousingdata.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we had the correlation, we prepared several scatter plots to take a look at the highly correlated metrics compared against the house value in order to see how the data compared overall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "data['Home Value'] = (data['Median Value'] * 1000)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a scatter plot of Charles River Variable(distance from the river) vs. value\n",
    "crimeRate = data.iloc[:,0]\n",
    "value = data.iloc[:,13]\n",
    "\n",
    "plt.scatter(crimeRate, value, color = '#545E45')\n",
    "\n",
    "#label\n",
    "plt.title('Crime Rate vs Value')\n",
    "plt.xlabel('Crime Rate')\n",
    "plt.ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a scatter plot of Charles River Variable(distance from the river) vs. value\n",
    "charlesRiver = data.iloc[:,3]\n",
    "value = data.iloc[:,13]\n",
    "\n",
    "plt.scatter(charlesRiver, value, color = '#8D2B00')\n",
    "\n",
    "#label\n",
    "plt.title('Distance from Charles River vs Value')\n",
    "plt.xlabel('Distance')\n",
    "plt.ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a scatter plot of Nitric Oxide Concentration vs. value\n",
    "nitricOxide = data.iloc[:,4]\n",
    "value = data.iloc[:,13]\n",
    "\n",
    "plt.scatter(nitricOxide, value, color = '#BE6731')\n",
    "\n",
    "#label\n",
    "plt.title('Nitric Oxide vs Value')\n",
    "plt.xlabel('Nitric')\n",
    "plt.ylabel('Value')\n",
    "plt.savefig(\"output/NitricOxideVsValue.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a scatter plot of rooms vs. value\n",
    "rooms = data.iloc[:,5]\n",
    "value = data.iloc[:,13]\n",
    "\n",
    "plt.scatter(rooms, value, color = '#76704C')\n",
    "\n",
    "\n",
    "#label\n",
    "plt.title('Number of Rooms vs Value')\n",
    "plt.xlabel('Rooms')\n",
    "plt.ylabel('Value')\n",
    "plt.savefig(\"output/NumberOfRoomsVsValue.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a scatter plot of age vs. value\n",
    "age = data.iloc[:,6]\n",
    "value = data.iloc[:,13]\n",
    "\n",
    "plt.scatter(age, value, color = '#B55119')\n",
    "\n",
    "#label\n",
    "plt.title('Age of Home vs Value')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Value')\n",
    "plt.savefig(\"output/AgeVsValue.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a scatter plot of % lower status vs. value\n",
    "lowerStatus = data.iloc[:,11]\n",
    "value = data.iloc[:,13]\n",
    "\n",
    "plt.scatter(lowerStatus, value, color = '#1F2D16')\n",
    "\n",
    "#label\n",
    "plt.title('% Lower Status vs Value')\n",
    "plt.xlabel('Status')\n",
    "plt.ylabel('Value')\n",
    "plt.savefig(\"output/StatusVsValue.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building The Model\n",
    "\n",
    "After our exploratory analysis we started to work on our model. After discussing our data with our instructor Dom, we went with a multiple linear regression neural network using MSE (Mean Squared Error) and R Squared Score to rate the models efficacy at his suggestion.\n",
    "\n",
    "After researching how to modify a neural network to fit our needs, we went with a sequential model using relu activation for the analysis and had a single output node using linear activation. As suggested, we had the model use mean squared error as the primary output in order to help us troubleshoot and get a rough gauge of the models performance.\n",
    "\n",
    "It was at this point we realized we had not yet standardized our data, which is a general best practice due to the different ways our data is being measured which makes comparison difficult. Instead of just standardizing it we decided to compare pre/post standardization model performance in order to quantify the level of impact that standardization would have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Model Before Standardizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from math import sqrt\n",
    "\n",
    "# Keras specific\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the data to X and y\n",
    "X = data[['Crime Rate','Residential Land Zone','Non-retail business acres','Charles River Variable','Nitric Oxide Concentration',\n",
    "             'Rooms','Age','Distance','Accessiblity to Highway','Tax Rate','Pupil-Teacher ratio','% lower status']]\n",
    "y = data['Median Value'].values.reshape(-1,1)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to create training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=150)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the model\n",
    "model = Sequential()\n",
    "model.add(Dense(36, input_dim=12, activation= \"relu\"))\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
    "model.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a test preciction to see if we're on the right track\n",
    "row = 3\n",
    "test_row = X_test.iloc[row, :]\n",
    "test_row_array = np.array(test_row).reshape(1, 12)\n",
    "test_row_array.shape\n",
    "\n",
    "print(f'Prediction = {model.predict(test_row_array)}')\n",
    "print(f'Actual = {y_test[row]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use X_test to predict y, and use that to calculate our R score\n",
    "y_test_predict = model.predict(X_test)\n",
    "r2_score(y_test, y_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R2 score for our model is not very reliable at a .37 which would put it in the higher of the of low reliability range for a model. Next we will standarize our data and then see if that improves our R score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model With Standardized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = StandardScaler().fit(X_train)\n",
    "y_scaler = StandardScaler().fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "y_train_scaled = y_scaler.transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_scaled = Sequential()\n",
    "model_scaled.add(Dense(36, input_dim=12, activation= \"relu\"))\n",
    "model_scaled.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_scaled.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
    "model_scaled.fit(X_train_scaled, y_train_scaled, epochs=100)\n",
    "\n",
    "#r2_score(y_train_scaled, y_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use X_test to predict y, and use that to calculate our R score\n",
    "y_test_scaled_predict = model_scaled.predict(X_test_scaled)\n",
    "r2_score(y_test_scaled, y_test_scaled_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model\n",
    "model_scaled.evaluate(X_test_scaled,y_test_scaled, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was a stark difference between the two models, showing that standardizing the data clearly improved performance of the model with the MSE decreasing considerably, and the R2 score improving majorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tinkering with the Data\n",
    "\n",
    "Once we standardized the data we took a look to see if removing some columns of data would help to improve the performance of the model. Looking at our Seaborn Heatmap we saw that both the Charles River and Distance inputs has very low correlation to house value. We dropped those two columns from our data, and created a new model using the same criteria to measure the overall impact on performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "manipulated_data = data.copy()\n",
    "\n",
    "manipulated_data = manipulated_data.drop([\"Charles River Variable\", 'Distance'],axis=1)\n",
    "manipulated_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_manipulated = manipulated_data[['Crime Rate','Residential Land Zone','Non-retail business acres','Nitric Oxide Concentration',\n",
    "             'Rooms','Age','Accessiblity to Highway','Tax Rate','Pupil-Teacher ratio','% lower status']]\n",
    "y_manipulated = manipulated_data['Median Value'].values.reshape(-1,1)\n",
    "#print(X.shape, y.shape)\n",
    "\n",
    "X_train_manipulated, X_test_manipulated, y_train_manipulated, y_test_manipulated = train_test_split(X_manipulated, y_manipulated, random_state=150)\n",
    "#print(X_train.shape)\n",
    "#print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler_manipulated = StandardScaler().fit(X_train_manipulated)\n",
    "y_scaler_manipulated = StandardScaler().fit(y_train_manipulated)\n",
    "\n",
    "X_train_scaled_manipulated = X_scaler_manipulated.transform(X_train_manipulated)\n",
    "X_test_scaled_manipulated = X_scaler_manipulated.transform(X_test_manipulated)\n",
    "y_train_scaled_manipulated = y_scaler_manipulated.transform(y_train_manipulated)\n",
    "y_test_scaled_manipulated = y_scaler_manipulated.transform(y_test_manipulated)\n",
    "\n",
    "model_manipulated = Sequential()\n",
    "model_manipulated.add(Dense(30, input_dim=10, activation= \"relu\"))\n",
    "\n",
    "model_manipulated.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_manipulated.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
    "model_manipulated.fit(X_train_scaled_manipulated, y_train_scaled_manipulated, epochs=100)\n",
    "\n",
    "model_manipulated.evaluate(X_test_scaled_manipulated,y_test_scaled_manipulated, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use X_test to predict y, and use that to calculate our R score\n",
    "y_test_scaled_manip_predict = model_manipulated.predict(X_test_scaled_manipulated)\n",
    "r2_score(y_test_scaled_manipulated, y_test_scaled_manip_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found removing those two columns did have a mild impact on performance, however upon repeated running of the model we found that the results would change seemingly at random. This led us to believe that removing the two columns was not meaningfully different.\n",
    "\n",
    "Next we tried selected the top 6 columns based on correlation to see if hand selecting the data would be more beneficial. We went Non-retail business acres, Nitric Oxide Concentration, Rooms, Tax Rate, Pupil-Teacher ratio, % lower status as our columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cherrypicked = data[['Non-retail business acres','Nitric Oxide Concentration',\n",
    "             'Rooms','Tax Rate','Pupil-Teacher ratio','% lower status']]\n",
    "y_cherrypicked = data['Median Value'].values.reshape(-1,1)\n",
    "#print(X.shape, y.shape)\n",
    "\n",
    "X_train_cherrypicked, X_test_cherrypicked, y_train_cherrypicked, y_test_cherrypicked = train_test_split(X_cherrypicked, y_cherrypicked, random_state=150)\n",
    "#print(X_train.shape)\n",
    "#print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler_cherrypicked = StandardScaler().fit(X_train_cherrypicked)\n",
    "y_scaler_cherrypicked = StandardScaler().fit(y_train_cherrypicked)\n",
    "\n",
    "X_train_scaled_cherrypicked = X_scaler_cherrypicked.transform(X_train_cherrypicked)\n",
    "X_test_scaled_cherrypicked = X_scaler_cherrypicked.transform(X_test_cherrypicked)\n",
    "y_train_scaled_cherrypicked = y_scaler_cherrypicked.transform(y_train_cherrypicked)\n",
    "y_test_scaled_cherrypicked = y_scaler_cherrypicked.transform(y_test_cherrypicked)\n",
    "\n",
    "model_cherrypicked = Sequential()\n",
    "model_cherrypicked.add(Dense(18, input_dim=6, activation= \"relu\"))\n",
    "model_cherrypicked.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_cherrypicked.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
    "model_cherrypicked.fit(X_train_scaled_cherrypicked, y_train_scaled_cherrypicked, epochs=100)\n",
    "\n",
    "model_cherrypicked.evaluate(X_test_scaled_cherrypicked,y_test_scaled_cherrypicked, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use X_test to predict y, and use that to calculate our R score\n",
    "y_test_scaled_cherrypicked_predict = model_cherrypicked.predict(X_test_scaled_cherrypicked)\n",
    "r2_score(y_test_scaled_cherrypicked, y_test_scaled_cherrypicked_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we saw the results of the test change upon running them, with only slight differences in performance. After seeing similar changes to simply dropping the two columns, it seemed wiser to keep the data set as is, in order to preserve the integrity of our data set.\n",
    "Next we decided to compare the data set cleaning methods we used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a model using a data set where we instead drop the rows contianing nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_drop = data_dropna[['Crime Rate','Residential Land Zone','Non-retail business acres','Charles River Variable','Nitric Oxide Concentration',\n",
    "             'Rooms','Age','Distance','Accessiblity to Highway','Tax Rate','Pupil-Teacher ratio','% lower status']]\n",
    "y_drop = data_dropna['Median Value'].values.reshape(-1,1)\n",
    "\n",
    "X_train_drop, X_test_drop, y_train_drop, y_test_drop = train_test_split(X_drop, y_drop, random_state=150)\n",
    "\n",
    "X_scaler_drop = StandardScaler().fit(X_train_drop)\n",
    "y_scaler_drop = StandardScaler().fit(y_train_drop)\n",
    "\n",
    "X_train_scaled_drop = X_scaler_drop.transform(X_train_drop)\n",
    "X_test_scaled_drop = X_scaler_drop.transform(X_test_drop)\n",
    "y_train_scaled_drop = y_scaler_drop.transform(y_train_drop)\n",
    "y_test_scaled_drop = y_scaler_drop.transform(y_test_drop)\n",
    "\n",
    "model_drop = Sequential()\n",
    "model_drop.add(Dense(36, input_dim=12, activation= \"relu\"))\n",
    "model_drop.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_drop.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
    "model_drop.fit(X_train_scaled_drop, y_train_scaled_drop, epochs=100)\n",
    "\n",
    "model_drop.evaluate(X_test_scaled_drop,y_test_scaled_drop, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_scaled_predict_drop = model_drop.predict(X_test_scaled_drop)\n",
    "r2_score(y_test_scaled_drop, y_test_scaled_predict_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had assumed that providing additional inputs using the imputation method would actually improve the models performance, but we found that wasnâ€™t the case at all. The data set where we had dropped all null values had a much higher R2 score and a lower MSE. \n",
    "\n",
    "Lastly we wanted to compare what the performance looked like if we removed the outliers from the dataset. For this analysis we used the drop NA data since it was more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "data_remove_outliers = data_dropna.copy()\n",
    "data_remove_outliers = data_remove_outliers[(np.abs(stats.zscore(data_remove_outliers)) < 3).all(axis=1)]\n",
    "data_remove_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_outlier = data_remove_outliers[['Crime Rate','Residential Land Zone','Non-retail business acres','Charles River Variable','Nitric Oxide Concentration',\n",
    "             'Rooms','Age','Distance','Accessiblity to Highway','Tax Rate','Pupil-Teacher ratio','% lower status']]\n",
    "y_outlier = data_remove_outliers['Median Value'].values.reshape(-1,1)\n",
    "\n",
    "X_train_outlier, X_test_outlier, y_train_outlier, y_test_outlier = train_test_split(X_outlier, y_outlier, random_state=150)\n",
    "\n",
    "X_scaler_outlier = StandardScaler().fit(X_train_outlier)\n",
    "y_scaler_outlier = StandardScaler().fit(y_train_outlier)\n",
    "\n",
    "X_train_scaled_outlier = X_scaler_outlier.transform(X_train_outlier)\n",
    "X_test_scaled_outlier = X_scaler_outlier.transform(X_test_outlier)\n",
    "y_train_scaled_outlier = y_scaler_outlier.transform(y_train_outlier)\n",
    "y_test_scaled_outlier = y_scaler_outlier.transform(y_test_outlier)\n",
    "\n",
    "model_outlier = Sequential()\n",
    "model_outlier.add(Dense(36, input_dim=12, activation= \"relu\"))\n",
    "model_outlier.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_outlier.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
    "model_outlier.fit(X_train_scaled_outlier, y_train_scaled_outlier, epochs=100)\n",
    "\n",
    "model_drop.evaluate(X_test_scaled_drop,y_test_scaled_drop, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_scaled_predict_outlier = model_outlier.predict(X_test_scaled_outlier)\n",
    "r2_score(y_test_scaled_outlier, y_test_scaled_predict_outlier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After which we saw that removing outliers did not significantly move the needly on either the MSE or R2 score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion.\n",
    "\n",
    "With our model getting a MSE of 0.10 and a R2 score of 0.90 we found that the model could be used to somewhat accurately predict the value of a house based on the provided inputs. We also found that our attempts at manipulating the data proved ultimately proved counter productive as each time we manipulated the data it either had no change, or hurt performance of our model. With the only exception being the standardization of the model.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
